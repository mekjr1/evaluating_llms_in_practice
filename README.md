# Evaluating LLMs in Practice
## A Comprehensive Tutorial Series on LLM Evaluation Metrics

[![GitHub Stars](https://img.shields.io/github/stars/mekjr1/evaluating_llms_in_practice?style=social)](https://github.com/mekjr1/evaluating_llms_in_practice)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains a comprehensive tutorial series on evaluating Large Language Models in practical applications. Through hands-on Jupyter notebooks, mathematical foundations, and real-world experiments, learn how to properly assess LLM performance.

## ğŸ“š Tutorial Series Overview

This is a **7-part series** covering everything from basic metrics to advanced evaluation pipelines:

1. **Part 1**: Why Accuracy Fails & Basic BLEU/ROUGE â€” âœ… Available
2. **Part 2**: BLEU and ROUGE Deep Dive & Math Foundations â€” âœ… Available
3. **Part 3**: Perplexity and Language Model Evaluation â€” âœ… Available
4. **Part 4**: Evaluating Retrieval-Augmented LLMs (RAG) â€” âœ… Available
5. **Part 5**: A/B Testing for LLM Applications â€” âœ… Available
6. **Part 6**: Human Evaluation & Preference Modeling â€” âœ… Available
7. **Part 7**: Holistic LLM Evaluation â€” âœ… Available

## ğŸš€ Quick Start

### Part 1: BLEU and ROUGE Introduction
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-1-bleu_and_rouge/bleu_and_rouge.ipynb?hl=en#runtime_type=gpu)

Introduction to LLM evaluation fundamentals:
- **Why accuracy/F1 fail** for text generation tasks
- **Basic BLEU and ROUGE concepts** with intuitive explanations
- **Simple model comparison** using DistilBART vs BART-Base
- **N-gram overlap visualization** and highlighting techniques
- **Comparative analysis** with clear tabular summaries

**ğŸ“ Location**: `part-1-bleu_and_rouge/bleu_and_rouge.ipynb`

### Part 2: BLEU and ROUGE Deep Dive
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-2-bleu_rouge_deep_dive/bleu_rouge_deep_dive.ipynb?hl=en#runtime_type=gpu)

**Medium-ready comprehensive tutorial** featuring:
- **Mathematical foundations** with LaTeX formulas
- **Step-by-step manual calculations** showing metric internals
- **Multiple experiments**: toy examples â†’ real models â†’ failure cases
- **Rich visualizations** with 4-panel analysis charts
- **Critical evaluation** of strengths and weaknesses
- **Best practices** for practical applications

**ğŸ“ Location**: `part-2-bleu_rouge_deep_dive/bleu_rouge_deep_dive.ipynb`

### Part 3: Perplexity and Language Model Evaluation
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-3-perplexity/perplexity.ipynb?hl=en#runtime_type=gpu)

Comprehensive guide to perplexity metrics:
- **Mathematical foundations** of perplexity and cross-entropy
- **Practical implementation** with real language models
- **Model comparison** across different architectures
- **Understanding surprisal** and information theory concepts

**ğŸ“ Location**: `part-3-perplexity/perplexity.ipynb`

### Part 4: Evaluating Retrieval-Augmented LLMs (RAG)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-4-rag-eval/rag_eval.ipynb?hl=en#runtime_type=gpu)

Essential metrics for RAG system evaluation:
- **Precision@k, Recall@k, Hit@k** for retrieval quality
- **Mean Reciprocal Rank (MRR)** for ranking assessment
- **Normalized Discounted Cumulative Gain (nDCG)** for relevance scoring
- **Practical implementation** with simulated datasets

**ğŸ“ Location**: `part-4-rag-eval/rag_eval.ipynb`

### Part 5: A/B Testing for LLM Applications
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-5-ab-testing/ab_testing.ipynb?hl=en#runtime_type=gpu)

Statistical methods for comparing LLM variants:
- **A/B testing design** for LLM applications
- **Two-proportion z-tests** for significance testing
- **User satisfaction metrics** and conversion analysis
- **Statistical significance** vs practical significance

**ğŸ“ Location**: `part-5-ab-testing/ab_testing.ipynb`

### Part 6: Human Evaluation & Preference Modeling
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-6-human-eval/human_eval.ipynb?hl=en#runtime_type=gpu)

Human-in-the-loop evaluation techniques:
- **Pairwise preference collection** and aggregation
- **Inter-rater agreement** and reliability metrics
- **Preference modeling** from human feedback
- **Simulation approaches** for preference data

**ğŸ“ Location**: `part-6-human-eval/human_eval.ipynb`

### Part 7: Holistic LLM Evaluation
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-7-holistic-eval/holistic_eval.ipynb?hl=en#runtime_type=gpu)

Comprehensive evaluation across multiple dimensions:
- **Multi-task performance aggregation**
- **Safety, fairness, and robustness metrics**
- **Cross-task correlation analysis**
- **Holistic scoring methodologies**

**ğŸ“ Location**: `part-7-holistic-eval/holistic_eval.ipynb`

## ğŸ¯ What You'll Learn

### Core Concepts
- Why accuracy/F1 don't work for generative models
- N-gram overlap metrics and their limitations
- Perplexity and information-theoretic measures
- Retrieval metrics for RAG systems
- Statistical testing for model comparison
- Human preference modeling and aggregation

### Practical Skills
- Implementing evaluation metrics from scratch
- Comparing multiple LLM models systematically
- Creating meaningful visualizations of metric results
- Designing and analyzing A/B tests
- Collecting and processing human feedback
- Building holistic evaluation pipelines

### Advanced Topics
- Mathematical foundations of evaluation metrics
- Correlation analysis between different metrics
- Failure case analysis and edge cases
- Statistical significance vs practical significance
- Inter-rater agreement and reliability
- Multi-dimensional model assessment

## ğŸ› ï¸ Installation & Setup

### Local Setup
```bash
git clone https://github.com/mekjr1/evaluating_llms_in_practice.git
cd evaluating_llms_in_practice
pip install transformers datasets evaluate rouge-score nltk matplotlib pandas seaborn
```

### Google Colab (Recommended)
Simply click the **"Open In Colab"** badges above! Each notebook is configured with:
- âœ… GPU runtime for faster model inference
- âœ… Automatic dependency installation
- âœ… Pre-configured environment

## ğŸ“Š Featured Experiments

### Text Generation Evaluation (Parts 1-2)
- **Model Comparisons**: DistilBART, BART-Base, T5-Small
- **Length Impact Analysis**: Short vs long summaries
- **Failure Case Studies**: When metrics mislead

### Language Model Assessment (Part 3)
- **Perplexity Calculations**: Cross-entropy and surprisal
- **Model Architecture Comparison**: Different language model types
- **Information Theory Applications**: Bits per character analysis

### RAG System Evaluation (Part 4)
- **Retrieval Quality Metrics**: Precision@k, Recall@k, Hit@k
- **Ranking Assessment**: Mean Reciprocal Rank (MRR)
- **Relevance Scoring**: Normalized Discounted Cumulative Gain

### Statistical Testing (Part 5)
- **A/B Test Design**: User satisfaction experiments
- **Significance Testing**: Two-proportion z-tests
- **Effect Size Analysis**: Practical vs statistical significance

### Human Evaluation (Part 6)
- **Preference Collection**: Pairwise comparison systems
- **Inter-rater Reliability**: Agreement measurement
- **Aggregation Methods**: Combining multiple human judgments

### Holistic Assessment (Part 7)
- **Multi-task Evaluation**: Performance across diverse tasks
- **Safety and Fairness**: Bias and robustness metrics
- **Composite Scoring**: Weighted evaluation frameworks

## ï¿½ Repository Structure

```
evaluating_llms_in_practice/
â”œâ”€â”€ part-1-bleu_and_rouge/          # Introduction to BLEU and ROUGE
â”‚   â””â”€â”€ bleu_and_rouge.ipynb
â”œâ”€â”€ part-2-bleu_rouge_deep_dive/    # Comprehensive BLEU/ROUGE analysis
â”‚   â”œâ”€â”€ bleu_rouge_deep_dive.ipynb
â”‚   â””â”€â”€ bleu_rouge_deep_dive_backup.ipynb
â”œâ”€â”€ part-3-perplexity/              # Perplexity and language modeling
â”‚   â”œâ”€â”€ perplexity.ipynb
â”‚   â””â”€â”€ perplexity_executed.ipynb
â”œâ”€â”€ part-4-rag-eval/                # RAG system evaluation
â”‚   â””â”€â”€ rag_eval.ipynb
â”œâ”€â”€ part-5-ab-testing/              # A/B testing for LLMs
â”‚   â””â”€â”€ ab_testing.ipynb
â”œâ”€â”€ part-6-human-eval/              # Human evaluation methods
â”‚   â””â”€â”€ human_eval.ipynb
â”œâ”€â”€ part-7-holistic-eval/           # Holistic evaluation approaches
â”‚   â””â”€â”€ holistic_eval.ipynb
â”œâ”€â”€ medium-posts/                   # Medium article drafts
â”‚   â”œâ”€â”€ part3_perplexity_medium_post.md
â”‚   â”œâ”€â”€ part4_rag_eval_medium_post.md
â”‚   â”œâ”€â”€ part5_ab_testing_medium_post.md
â”‚   â”œâ”€â”€ part6_human_eval_medium_post.md
â”‚   â””â”€â”€ part7_holistic_eval_medium_post.md
â””â”€â”€ README.md
```

## ï¿½ğŸ“ˆ Visualizations

Each notebook includes rich visualizations:
- **Bar charts**: Model performance comparisons
- **Correlation plots**: BLEU vs ROUGE relationships
- **Length impact charts**: How summary length affects scores
- **Distribution plots**: Score breakdowns across metrics
- **Statistical test results**: A/B testing outcomes
- **Preference aggregation**: Human evaluation summaries

## ğŸ“ Learning Path

### Beginner (Start Here)
1. **Part 1**: Learn why traditional accuracy fails for LLMs and basic BLEU/ROUGE
2. **Part 3**: Understand perplexity and language model evaluation
3. Run simple experiments with CNN/DailyMail dataset
4. Understand metric outputs and visual interpretations

### Intermediate 
1. **Part 2**: Deep dive into BLEU/ROUGE mathematical foundations
2. **Part 4**: Learn RAG system evaluation metrics
3. **Part 5**: Design and analyze A/B tests
4. Implement manual calculations and real model comparisons

### Advanced
1. **Part 6**: Master human evaluation and preference modeling
2. **Part 7**: Build holistic evaluation frameworks
3. Develop production evaluation systems
4. Conduct comprehensive model assessments

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to:
- Report bugs or issues
- Suggest new experiments or visualizations
- Add support for additional models
- Improve documentation and explanations

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Connect

- **GitHub**: [@mekjr1](https://github.com/mekjr1)
- **Repository**: [evaluating_llms_in_practice](https://github.com/mekjr1/evaluating_llms_in_practice)

---

â­ **Star this repo** if you find it helpful for your LLM evaluation journey!
