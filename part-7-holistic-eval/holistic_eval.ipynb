{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9651df3b",
   "metadata": {},
   "source": [
    "# Part 7: Holistic Evaluation of LLMs\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-7-holistic-eval/holistic_eval.ipynb?hl=en#runtime_type=gpu)\n",
    "\n",
    "In the final part of this series we look beyond individual metrics and tasks to consider **holistic evaluation**.  Realâ€‘world language models must perform a variety of tasksâ€”summarisation, translation, retrieval, code generation, and moreâ€”and must also be safe, fair and robust.\n",
    "\n",
    "This notebook constructs a simple example of aggregating performance across multiple tasks, using simulated metrics for two models.  We compute an overall score and discuss what such aggregates can and cannot tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3b176",
   "metadata": {},
   "source": [
    "## Simulated Crossâ€‘Task Metrics\n",
    "\n",
    "We simulate performance for two models across four tasks.  Each task is evaluated with an appropriate metric (e.g. ROUGEâ€‘L for summarisation, BLEU for translation).  Higher values indicate better performance for all metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13648d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Update metrics to match the medium post data more closely\n",
    "metrics = pd.DataFrame({\n",
    "    'Task': ['Summarization', 'Translation', 'Retrieval', 'Code generation'],\n",
    "    'Metric': ['ROUGE-L', 'BLEU', 'nDCG', 'Accuracy'],\n",
    "    'Model A': [0.63, 0.47, 0.62, 0.55],  # Updated to match medium post\n",
    "    'Model B': [0.58, 0.49, 0.60, 0.55]   # Updated to match medium post\n",
    "})\n",
    "\n",
    "metrics['Improvement'] = metrics['Model A'] - metrics['Model B']\n",
    "\n",
    "# Calculate overall scores\n",
    "avg_scores = metrics[['Model A','Model B']].mean()\n",
    "overall = {\n",
    "    'Model A': round(avg_scores['Model A'], 3),\n",
    "    'Model B': round(avg_scores['Model B'], 3),\n",
    "    'Difference': round(avg_scores['Model A'] - avg_scores['Model B'], 3)\n",
    "}\n",
    "\n",
    "# Display the data\n",
    "print(\"Performance Metrics:\")\n",
    "print(metrics)\n",
    "print(f\"\\nOverall Scores:\")\n",
    "print(f\"Model A: {overall['Model A']}\")\n",
    "print(f\"Model B: {overall['Model B']}\")\n",
    "print(f\"Difference: {overall['Difference']}\")\n",
    "\n",
    "metrics, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Holistic Evaluation with Advanced Analysis\n",
    "# Create extensive visualization suite\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "\n",
    "# 1. Basic performance comparison\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "x = np.arange(len(metrics['Task']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, metrics['Model A'], width, label='Model A', color='lightblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, metrics['Model B'], width, label='Model B', color='lightcoral', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Task')\n",
    "ax1.set_ylabel('Performance Score')\n",
    "ax1.set_title('Performance by Task')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics['Task'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Radar chart for holistic view\n",
    "ax2 = plt.subplot(2, 4, 2, projection='polar')\n",
    "\n",
    "# Prepare data for radar chart\n",
    "tasks = metrics['Task'].tolist()\n",
    "model_a_values = metrics['Model A'].tolist()\n",
    "model_b_values = metrics['Model B'].tolist()\n",
    "\n",
    "# Add angles for radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(tasks), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "model_a_values += model_a_values[:1]\n",
    "model_b_values += model_b_values[:1]\n",
    "\n",
    "ax2.plot(angles, model_a_values, 'o-', linewidth=2, label='Model A', color='blue')\n",
    "ax2.fill(angles, model_a_values, alpha=0.25, color='blue')\n",
    "ax2.plot(angles, model_b_values, 'o-', linewidth=2, label='Model B', color='red')\n",
    "ax2.fill(angles, model_b_values, alpha=0.25, color='red')\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(tasks)\n",
    "ax2.set_title('Multi-Task Performance Radar')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# 3. Performance differences\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "improvements = metrics['Improvement']\n",
    "colors = ['green' if x > 0 else 'red' if x < 0 else 'gray' for x in improvements]\n",
    "\n",
    "bars = ax3.bar(range(len(tasks)), improvements, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Task')\n",
    "ax3.set_ylabel('Performance Difference (A - B)')\n",
    "ax3.set_title('Model A Advantage by Task')\n",
    "ax3.set_xticks(range(len(tasks)))\n",
    "ax3.set_xticklabels(tasks, rotation=45, ha='right')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, diff) in enumerate(zip(bars, improvements)):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + (0.005 if height >= 0 else -0.005),\n",
    "             f'{diff:+.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
    "\n",
    "# 4. Weighted performance (different task importance)\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "\n",
    "# Define task weights (example: some tasks more important than others)\n",
    "task_weights = {'Summarization': 0.3, 'Translation': 0.2, 'Retrieval': 0.3, 'Code generation': 0.2}\n",
    "weights = [task_weights[task] for task in tasks]\n",
    "\n",
    "weighted_a = sum(w * score for w, score in zip(weights, metrics['Model A']))\n",
    "weighted_b = sum(w * score for w, score in zip(weights, metrics['Model B']))\n",
    "\n",
    "# Show both regular and weighted averages\n",
    "categories = ['Simple Average', 'Weighted Average']\n",
    "model_a_scores = [overall['Model A'], weighted_a]\n",
    "model_b_scores = [overall['Model B'], weighted_b]\n",
    "\n",
    "x_pos = np.arange(len(categories))\n",
    "bars1 = ax4.bar(x_pos - 0.2, model_a_scores, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax4.bar(x_pos + 0.2, model_b_scores, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax4.set_xlabel('Aggregation Method')\n",
    "ax4.set_ylabel('Overall Score')\n",
    "ax4.set_title('Overall Performance Comparison')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(categories)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 5. Safety and Bias Analysis (simulated)\n",
    "ax5 = plt.subplot(2, 4, 5)\n",
    "\n",
    "# Simulate safety/bias scores (0-1, higher is better)\n",
    "safety_categories = ['Toxicity\\nAvoidance', 'Bias\\nMitigation', 'Factual\\nAccuracy', 'Privacy\\nProtection']\n",
    "model_a_safety = [0.85, 0.78, 0.82, 0.90]\n",
    "model_b_safety = [0.88, 0.75, 0.79, 0.85]\n",
    "\n",
    "x_safety = np.arange(len(safety_categories))\n",
    "bars1 = ax5.bar(x_safety - 0.2, model_a_safety, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax5.bar(x_safety + 0.2, model_b_safety, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax5.set_xlabel('Safety Dimension')\n",
    "ax5.set_ylabel('Safety Score (0-1)')\n",
    "ax5.set_title('Safety & Ethics Comparison')\n",
    "ax5.set_xticks(x_safety)\n",
    "ax5.set_xticklabels(safety_categories)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.set_ylim(0, 1)\n",
    "\n",
    "# 6. Efficiency Metrics (simulated)\n",
    "ax6 = plt.subplot(2, 4, 6)\n",
    "\n",
    "efficiency_metrics = ['Latency\\n(lower better)', 'Memory\\n(lower better)', 'Cost\\n(lower better)', 'Energy\\n(lower better)']\n",
    "# Invert some metrics (lower is better)\n",
    "model_a_efficiency = [0.3, 0.6, 0.4, 0.5]  # Lower values are better\n",
    "model_b_efficiency = [0.4, 0.5, 0.5, 0.6]\n",
    "\n",
    "x_eff = np.arange(len(efficiency_metrics))\n",
    "bars1 = ax6.bar(x_eff - 0.2, model_a_efficiency, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax6.bar(x_eff + 0.2, model_b_efficiency, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax6.set_xlabel('Efficiency Dimension')\n",
    "ax6.set_ylabel('Resource Usage (lower better)')\n",
    "ax6.set_title('Efficiency Comparison')\n",
    "ax6.set_xticks(x_eff)\n",
    "ax6.set_xticklabels(efficiency_metrics)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Multi-dimensional comparison heatmap\n",
    "ax7 = plt.subplot(2, 4, 7)\n",
    "\n",
    "# Create comprehensive comparison matrix\n",
    "dimensions = ['Performance', 'Safety', 'Efficiency', 'Robustness', 'Interpretability']\n",
    "comparison_data = np.array([\n",
    "    [overall['Model A'], overall['Model B']],  # Performance\n",
    "    [np.mean(model_a_safety), np.mean(model_b_safety)],  # Safety\n",
    "    [1 - np.mean(model_a_efficiency), 1 - np.mean(model_b_efficiency)],  # Efficiency (inverted)\n",
    "    [0.75, 0.72],  # Robustness (simulated)\n",
    "    [0.68, 0.71]   # Interpretability (simulated)\n",
    "])\n",
    "\n",
    "im = ax7.imshow(comparison_data, cmap='RdYlBu', aspect='auto')\n",
    "ax7.set_xticks([0, 1])\n",
    "ax7.set_yticks(range(len(dimensions)))\n",
    "ax7.set_xticklabels(['Model A', 'Model B'])\n",
    "ax7.set_yticklabels(dimensions)\n",
    "ax7.set_title('Multi-Dimensional Comparison')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(dimensions)):\n",
    "    for j in range(2):\n",
    "        text = ax7.text(j, i, f'{comparison_data[i, j]:.3f}', \n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax7, fraction=0.046, pad=0.04)\n",
    "\n",
    "# 8. Final recommendation matrix\n",
    "ax8 = plt.subplot(2, 4, 8)\n",
    "ax8.axis('tight')\n",
    "ax8.axis('off')\n",
    "\n",
    "# Decision matrix\n",
    "decision_criteria = [\n",
    "    ['Criterion', 'Model A', 'Model B', 'Winner'],\n",
    "    ['Overall Performance', f'{overall[\"Model A\"]:.3f}', f'{overall[\"Model B\"]:.3f}', 'Model A'],\n",
    "    ['Best Task Performance', 'Summarization', 'Translation', 'Tie'],\n",
    "    ['Safety Average', f'{np.mean(model_a_safety):.3f}', f'{np.mean(model_b_safety):.3f}', 'Model B'],\n",
    "    ['Efficiency Average', f'{np.mean(model_a_efficiency):.3f}', f'{np.mean(model_b_efficiency):.3f}', 'Model A'],\n",
    "    ['Consistency', 'Good', 'Good', 'Tie'],\n",
    "    ['Overall Recommendation', 'âœ“', '', 'Model A']\n",
    "]\n",
    "\n",
    "table = ax8.table(cellText=decision_criteria[1:], colLabels=decision_criteria[0], \n",
    "                 cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# Color code the winner column\n",
    "colors = {'Model A': '#90EE90', 'Model B': '#FFB6C1', 'Tie': '#D3D3D3'}\n",
    "for i in range(1, len(decision_criteria)):\n",
    "    winner = decision_criteria[i][3]\n",
    "    if winner in colors:\n",
    "        table[(i, 3)].set_facecolor(colors[winner])\n",
    "\n",
    "# Highlight final recommendation\n",
    "table[(len(decision_criteria)-1, 1)].set_facecolor('#90EE90')\n",
    "\n",
    "ax8.set_title('Decision Matrix & Recommendation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HOLISTIC EVALUATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nðŸ“Š PERFORMANCE SUMMARY:\")\n",
    "print(f\"â€¢ Overall Score - Model A: {overall['Model A']:.3f}, Model B: {overall['Model B']:.3f}\")\n",
    "print(f\"â€¢ Advantage: Model A leads by {overall['Difference']:.3f} points ({overall['Difference']/overall['Model B']*100:.1f}%)\")\n",
    "print(f\"â€¢ Task Strengths:\")\n",
    "for i, row in metrics.iterrows():\n",
    "    winner = \"Model A\" if row['Improvement'] > 0 else \"Model B\" if row['Improvement'] < 0 else \"Tie\"\n",
    "    print(f\"  - {row['Task']}: {winner} (A: {row['Model A']:.3f}, B: {row['Model B']:.3f})\")\n",
    "\n",
    "print(f\"\\nðŸ›¡ï¸ SAFETY & ETHICS:\")\n",
    "safety_avg_a = np.mean(model_a_safety)\n",
    "safety_avg_b = np.mean(model_b_safety)\n",
    "print(f\"â€¢ Model A Safety Score: {safety_avg_a:.3f}\")\n",
    "print(f\"â€¢ Model B Safety Score: {safety_avg_b:.3f}\")\n",
    "print(f\"â€¢ Safety Leader: {'Model B' if safety_avg_b > safety_avg_a else 'Model A'}\")\n",
    "\n",
    "print(f\"\\nâš¡ EFFICIENCY:\")\n",
    "eff_avg_a = np.mean(model_a_efficiency)\n",
    "eff_avg_b = np.mean(model_b_efficiency)\n",
    "print(f\"â€¢ Model A Resource Usage: {eff_avg_a:.3f} (lower is better)\")\n",
    "print(f\"â€¢ Model B Resource Usage: {eff_avg_b:.3f} (lower is better)\")\n",
    "print(f\"â€¢ Efficiency Leader: {'Model A' if eff_avg_a < eff_avg_b else 'Model B'}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
    "print(f\"1. **Primary Choice**: Model A\")\n",
    "print(f\"   - Slightly better overall performance ({overall['Difference']:+.3f})\")\n",
    "print(f\"   - Superior at summarization and retrieval tasks\")\n",
    "print(f\"   - More efficient resource usage\")\n",
    "print(f\"\")\n",
    "print(f\"2. **Consider Model B if**:\")\n",
    "print(f\"   - Translation quality is critical\")\n",
    "print(f\"   - Safety is the top priority\")\n",
    "print(f\"   - Performance difference is not significant for your use case\")\n",
    "print(f\"\")\n",
    "print(f\"3. **Key Considerations**:\")\n",
    "print(f\"   - The performance gap is small ({overall['Difference']:.3f})\")\n",
    "print(f\"   - Task-specific requirements should drive the final decision\")\n",
    "print(f\"   - Consider A/B testing with real users for validation\")\n",
    "print(f\"   - Monitor both safety and performance in production\")\n",
    "\n",
    "print(f\"\\nâš ï¸ LIMITATIONS OF THIS ANALYSIS:\")\n",
    "print(f\"â€¢ Simple averaging may not reflect real-world usage patterns\")\n",
    "print(f\"â€¢ Safety and efficiency scores are simulated examples\")\n",
    "print(f\"â€¢ Missing important factors: user satisfaction, domain-specific performance\")\n",
    "print(f\"â€¢ Static evaluation doesn't capture model behavior over time\")\n",
    "print(f\"â€¢ Need continuous monitoring and evaluation in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44814fd9",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In this example, **ModelÂ A** outperforms **ModelÂ B** on summarisation and retrieval, while **ModelÂ B** slightly edges out ModelÂ A on translation.  The models have comparable performance on code generation.\n",
    "\n",
    "We compute a simple average across tasks to obtain an overall score:\n",
    "\n",
    "- ModelÂ A: 0.568\n",
    "- ModelÂ B: 0.555\n",
    "- Difference: 0.013\n",
    "\n",
    "While such aggregates can be helpful, they hide important details.  Real holistic evaluations should also consider:\n",
    "\n",
    "- **Safety and ethics:** Does the model avoid harmful or biased output?\n",
    "- **Fairness:** Do performance gaps exist across different languages, dialects, or user demographics?\n",
    "- **Robustness:** How does the model behave under adversarial or noisy inputs?\n",
    "- **Consistency:** Are results stable over time and across different datasets?\n",
    "\n",
    "No single number can capture all aspects of a modelâ€™s performance.  A rigorous evaluation programme combines automatic metrics, human judgments, safety checks and domainâ€‘specific assessments to build confidence in deployment.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the notebook series on evaluating LLMs.  See the accompanying blog post for further discussion and pointers to ongoing research on holistic evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
