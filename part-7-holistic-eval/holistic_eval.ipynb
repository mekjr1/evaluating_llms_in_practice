{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9651df3b",
   "metadata": {},
   "source": [
    "# Part 7: Holistic Evaluation of LLMs\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-7-holistic-eval/holistic_eval.ipynb?hl=en#runtime_type=gpu)\n",
    "\n",
    "In the final part of this series we look beyond individual metrics and tasks to consider **holistic evaluation**.  Real‑world language models must perform a variety of tasks—summarisation, translation, retrieval, code generation, and more—and must also be safe, fair and robust.\n",
    "\n",
    "This notebook constructs a simple example of aggregating performance across multiple tasks, using simulated metrics for two models.  We compute an overall score and discuss what such aggregates can and cannot tell us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3b176",
   "metadata": {},
   "source": [
    "## Simulated Cross‑Task Metrics\n",
    "\n",
    "We simulate performance for two models across four tasks.  Each task is evaluated with an appropriate metric (e.g. ROUGE‑L for summarisation, BLEU for translation).  Higher values indicate better performance for all metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13648d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Update metrics to match the medium post data more closely\n",
    "metrics = pd.DataFrame({\n",
    "    'Task': ['Summarization', 'Translation', 'Retrieval', 'Code generation'],\n",
    "    'Metric': ['ROUGE-L', 'BLEU', 'nDCG', 'Accuracy'],\n",
    "    'Model A': [0.63, 0.47, 0.62, 0.55],  # Updated to match medium post\n",
    "    'Model B': [0.58, 0.49, 0.60, 0.55]   # Updated to match medium post\n",
    "})\n",
    "\n",
    "metrics['Improvement'] = metrics['Model A'] - metrics['Model B']\n",
    "\n",
    "# Calculate overall scores\n",
    "avg_scores = metrics[['Model A','Model B']].mean()\n",
    "overall = {\n",
    "    'Model A': round(avg_scores['Model A'], 3),\n",
    "    'Model B': round(avg_scores['Model B'], 3),\n",
    "    'Difference': round(avg_scores['Model A'] - avg_scores['Model B'], 3)\n",
    "}\n",
    "\n",
    "# Display the data\n",
    "print(\"Performance Metrics:\")\n",
    "print(metrics)\n",
    "print(f\"\\nOverall Scores:\")\n",
    "print(f\"Model A: {overall['Model A']}\")\n",
    "print(f\"Model B: {overall['Model B']}\")\n",
    "print(f\"Difference: {overall['Difference']}\")\n",
    "\n",
    "metrics, overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4bc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Holistic Evaluation with Advanced Analysis\n",
    "# Create extensive visualization suite\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "\n",
    "# 1. Basic performance comparison\n",
    "ax1 = plt.subplot(2, 4, 1)\n",
    "x = np.arange(len(metrics['Task']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, metrics['Model A'], width, label='Model A', color='lightblue', alpha=0.8)\n",
    "bars2 = ax1.bar(x + width/2, metrics['Model B'], width, label='Model B', color='lightcoral', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Task')\n",
    "ax1.set_ylabel('Performance Score')\n",
    "ax1.set_title('Performance by Task')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics['Task'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 2. Radar chart for holistic view\n",
    "ax2 = plt.subplot(2, 4, 2, projection='polar')\n",
    "\n",
    "# Prepare data for radar chart\n",
    "tasks = metrics['Task'].tolist()\n",
    "model_a_values = metrics['Model A'].tolist()\n",
    "model_b_values = metrics['Model B'].tolist()\n",
    "\n",
    "# Add angles for radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(tasks), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "model_a_values += model_a_values[:1]\n",
    "model_b_values += model_b_values[:1]\n",
    "\n",
    "ax2.plot(angles, model_a_values, 'o-', linewidth=2, label='Model A', color='blue')\n",
    "ax2.fill(angles, model_a_values, alpha=0.25, color='blue')\n",
    "ax2.plot(angles, model_b_values, 'o-', linewidth=2, label='Model B', color='red')\n",
    "ax2.fill(angles, model_b_values, alpha=0.25, color='red')\n",
    "\n",
    "ax2.set_xticks(angles[:-1])\n",
    "ax2.set_xticklabels(tasks)\n",
    "ax2.set_title('Multi-Task Performance Radar')\n",
    "ax2.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# 3. Performance differences\n",
    "ax3 = plt.subplot(2, 4, 3)\n",
    "improvements = metrics['Improvement']\n",
    "colors = ['green' if x > 0 else 'red' if x < 0 else 'gray' for x in improvements]\n",
    "\n",
    "bars = ax3.bar(range(len(tasks)), improvements, color=colors, alpha=0.7)\n",
    "ax3.set_xlabel('Task')\n",
    "ax3.set_ylabel('Performance Difference (A - B)')\n",
    "ax3.set_title('Model A Advantage by Task')\n",
    "ax3.set_xticks(range(len(tasks)))\n",
    "ax3.set_xticklabels(tasks, rotation=45, ha='right')\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, diff) in enumerate(zip(bars, improvements)):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + (0.005 if height >= 0 else -0.005),\n",
    "             f'{diff:+.3f}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
    "\n",
    "# 4. Weighted performance (different task importance)\n",
    "ax4 = plt.subplot(2, 4, 4)\n",
    "\n",
    "# Define task weights (example: some tasks more important than others)\n",
    "task_weights = {'Summarization': 0.3, 'Translation': 0.2, 'Retrieval': 0.3, 'Code generation': 0.2}\n",
    "weights = [task_weights[task] for task in tasks]\n",
    "\n",
    "weighted_a = sum(w * score for w, score in zip(weights, metrics['Model A']))\n",
    "weighted_b = sum(w * score for w, score in zip(weights, metrics['Model B']))\n",
    "\n",
    "# Show both regular and weighted averages\n",
    "categories = ['Simple Average', 'Weighted Average']\n",
    "model_a_scores = [overall['Model A'], weighted_a]\n",
    "model_b_scores = [overall['Model B'], weighted_b]\n",
    "\n",
    "x_pos = np.arange(len(categories))\n",
    "bars1 = ax4.bar(x_pos - 0.2, model_a_scores, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax4.bar(x_pos + 0.2, model_b_scores, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax4.set_xlabel('Aggregation Method')\n",
    "ax4.set_ylabel('Overall Score')\n",
    "ax4.set_title('Overall Performance Comparison')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(categories)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 5. Safety and Bias Analysis (simulated)\n",
    "ax5 = plt.subplot(2, 4, 5)\n",
    "\n",
    "# Simulate safety/bias scores (0-1, higher is better)\n",
    "safety_categories = ['Toxicity\\nAvoidance', 'Bias\\nMitigation', 'Factual\\nAccuracy', 'Privacy\\nProtection']\n",
    "model_a_safety = [0.85, 0.78, 0.82, 0.90]\n",
    "model_b_safety = [0.88, 0.75, 0.79, 0.85]\n",
    "\n",
    "x_safety = np.arange(len(safety_categories))\n",
    "bars1 = ax5.bar(x_safety - 0.2, model_a_safety, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax5.bar(x_safety + 0.2, model_b_safety, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax5.set_xlabel('Safety Dimension')\n",
    "ax5.set_ylabel('Safety Score (0-1)')\n",
    "ax5.set_title('Safety & Ethics Comparison')\n",
    "ax5.set_xticks(x_safety)\n",
    "ax5.set_xticklabels(safety_categories)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "ax5.set_ylim(0, 1)\n",
    "\n",
    "# 6. Efficiency Metrics (simulated)\n",
    "ax6 = plt.subplot(2, 4, 6)\n",
    "\n",
    "efficiency_metrics = ['Latency\\n(lower better)', 'Memory\\n(lower better)', 'Cost\\n(lower better)', 'Energy\\n(lower better)']\n",
    "# Invert some metrics (lower is better)\n",
    "model_a_efficiency = [0.3, 0.6, 0.4, 0.5]  # Lower values are better\n",
    "model_b_efficiency = [0.4, 0.5, 0.5, 0.6]\n",
    "\n",
    "x_eff = np.arange(len(efficiency_metrics))\n",
    "bars1 = ax6.bar(x_eff - 0.2, model_a_efficiency, 0.4, label='Model A', color='lightblue')\n",
    "bars2 = ax6.bar(x_eff + 0.2, model_b_efficiency, 0.4, label='Model B', color='lightcoral')\n",
    "\n",
    "ax6.set_xlabel('Efficiency Dimension')\n",
    "ax6.set_ylabel('Resource Usage (lower better)')\n",
    "ax6.set_title('Efficiency Comparison')\n",
    "ax6.set_xticks(x_eff)\n",
    "ax6.set_xticklabels(efficiency_metrics)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Multi-dimensional comparison heatmap\n",
    "ax7 = plt.subplot(2, 4, 7)\n",
    "\n",
    "# Create comprehensive comparison matrix\n",
    "dimensions = ['Performance', 'Safety', 'Efficiency', 'Robustness', 'Interpretability']\n",
    "comparison_data = np.array([\n",
    "    [overall['Model A'], overall['Model B']],  # Performance\n",
    "    [np.mean(model_a_safety), np.mean(model_b_safety)],  # Safety\n",
    "    [1 - np.mean(model_a_efficiency), 1 - np.mean(model_b_efficiency)],  # Efficiency (inverted)\n",
    "    [0.75, 0.72],  # Robustness (simulated)\n",
    "    [0.68, 0.71]   # Interpretability (simulated)\n",
    "])\n",
    "\n",
    "im = ax7.imshow(comparison_data, cmap='RdYlBu', aspect='auto')\n",
    "ax7.set_xticks([0, 1])\n",
    "ax7.set_yticks(range(len(dimensions)))\n",
    "ax7.set_xticklabels(['Model A', 'Model B'])\n",
    "ax7.set_yticklabels(dimensions)\n",
    "ax7.set_title('Multi-Dimensional Comparison')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(dimensions)):\n",
    "    for j in range(2):\n",
    "        text = ax7.text(j, i, f'{comparison_data[i, j]:.3f}', \n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax7, fraction=0.046, pad=0.04)\n",
    "\n",
    "# 8. Final recommendation matrix\n",
    "ax8 = plt.subplot(2, 4, 8)\n",
    "ax8.axis('tight')\n",
    "ax8.axis('off')\n",
    "\n",
    "# Decision matrix\n",
    "decision_criteria = [\n",
    "    ['Criterion', 'Model A', 'Model B', 'Winner'],\n",
    "    ['Overall Performance', f'{overall[\"Model A\"]:.3f}', f'{overall[\"Model B\"]:.3f}', 'Model A'],\n",
    "    ['Best Task Performance', 'Summarization', 'Translation', 'Tie'],\n",
    "    ['Safety Average', f'{np.mean(model_a_safety):.3f}', f'{np.mean(model_b_safety):.3f}', 'Model B'],\n",
    "    ['Efficiency Average', f'{np.mean(model_a_efficiency):.3f}', f'{np.mean(model_b_efficiency):.3f}', 'Model A'],\n",
    "    ['Consistency', 'Good', 'Good', 'Tie'],\n",
    "    ['Overall Recommendation', '✓', '', 'Model A']\n",
    "]\n",
    "\n",
    "table = ax8.table(cellText=decision_criteria[1:], colLabels=decision_criteria[0], \n",
    "                 cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1, 1.5)\n",
    "\n",
    "# Color code the winner column\n",
    "colors = {'Model A': '#90EE90', 'Model B': '#FFB6C1', 'Tie': '#D3D3D3'}\n",
    "for i in range(1, len(decision_criteria)):\n",
    "    winner = decision_criteria[i][3]\n",
    "    if winner in colors:\n",
    "        table[(i, 3)].set_facecolor(colors[winner])\n",
    "\n",
    "# Highlight final recommendation\n",
    "table[(len(decision_criteria)-1, 1)].set_facecolor('#90EE90')\n",
    "\n",
    "ax8.set_title('Decision Matrix & Recommendation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HOLISTIC EVALUATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\n📊 PERFORMANCE SUMMARY:\")\n",
    "print(f\"• Overall Score - Model A: {overall['Model A']:.3f}, Model B: {overall['Model B']:.3f}\")\n",
    "print(f\"• Advantage: Model A leads by {overall['Difference']:.3f} points ({overall['Difference']/overall['Model B']*100:.1f}%)\")\n",
    "print(f\"• Task Strengths:\")\n",
    "for i, row in metrics.iterrows():\n",
    "    winner = \"Model A\" if row['Improvement'] > 0 else \"Model B\" if row['Improvement'] < 0 else \"Tie\"\n",
    "    print(f\"  - {row['Task']}: {winner} (A: {row['Model A']:.3f}, B: {row['Model B']:.3f})\")\n",
    "\n",
    "print(f\"\\n🛡️ SAFETY & ETHICS:\")\n",
    "safety_avg_a = np.mean(model_a_safety)\n",
    "safety_avg_b = np.mean(model_b_safety)\n",
    "print(f\"• Model A Safety Score: {safety_avg_a:.3f}\")\n",
    "print(f\"• Model B Safety Score: {safety_avg_b:.3f}\")\n",
    "print(f\"• Safety Leader: {'Model B' if safety_avg_b > safety_avg_a else 'Model A'}\")\n",
    "\n",
    "print(f\"\\n⚡ EFFICIENCY:\")\n",
    "eff_avg_a = np.mean(model_a_efficiency)\n",
    "eff_avg_b = np.mean(model_b_efficiency)\n",
    "print(f\"• Model A Resource Usage: {eff_avg_a:.3f} (lower is better)\")\n",
    "print(f\"• Model B Resource Usage: {eff_avg_b:.3f} (lower is better)\")\n",
    "print(f\"• Efficiency Leader: {'Model A' if eff_avg_a < eff_avg_b else 'Model B'}\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
    "print(f\"1. **Primary Choice**: Model A\")\n",
    "print(f\"   - Slightly better overall performance ({overall['Difference']:+.3f})\")\n",
    "print(f\"   - Superior at summarization and retrieval tasks\")\n",
    "print(f\"   - More efficient resource usage\")\n",
    "print(f\"\")\n",
    "print(f\"2. **Consider Model B if**:\")\n",
    "print(f\"   - Translation quality is critical\")\n",
    "print(f\"   - Safety is the top priority\")\n",
    "print(f\"   - Performance difference is not significant for your use case\")\n",
    "print(f\"\")\n",
    "print(f\"3. **Key Considerations**:\")\n",
    "print(f\"   - The performance gap is small ({overall['Difference']:.3f})\")\n",
    "print(f\"   - Task-specific requirements should drive the final decision\")\n",
    "print(f\"   - Consider A/B testing with real users for validation\")\n",
    "print(f\"   - Monitor both safety and performance in production\")\n",
    "\n",
    "print(f\"\\n⚠️ LIMITATIONS OF THIS ANALYSIS:\")\n",
    "print(f\"• Simple averaging may not reflect real-world usage patterns\")\n",
    "print(f\"• Safety and efficiency scores are simulated examples\")\n",
    "print(f\"• Missing important factors: user satisfaction, domain-specific performance\")\n",
    "print(f\"• Static evaluation doesn't capture model behavior over time\")\n",
    "print(f\"• Need continuous monitoring and evaluation in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44814fd9",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In this example, **Model A** outperforms **Model B** on summarisation and retrieval, while **Model B** slightly edges out Model A on translation.  The models have comparable performance on code generation.\n",
    "\n",
    "We compute a simple average across tasks to obtain an overall score:\n",
    "\n",
    "- Model A: 0.568\n",
    "- Model B: 0.555\n",
    "- Difference: 0.013\n",
    "\n",
    "While such aggregates can be helpful, they hide important details.  Real holistic evaluations should also consider:\n",
    "\n",
    "- **Safety and ethics:** Does the model avoid harmful or biased output?\n",
    "- **Fairness:** Do performance gaps exist across different languages, dialects, or user demographics?\n",
    "- **Robustness:** How does the model behave under adversarial or noisy inputs?\n",
    "- **Consistency:** Are results stable over time and across different datasets?\n",
    "\n",
    "No single number can capture all aspects of a model’s performance.  A rigorous evaluation programme combines automatic metrics, human judgments, safety checks and domain‑specific assessments to build confidence in deployment.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the notebook series on evaluating LLMs.  See the accompanying blog post for further discussion and pointers to ongoing research on holistic evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
