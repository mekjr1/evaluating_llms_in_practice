{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a08056",
   "metadata": {},
   "source": [
    "# Part 4: Evaluating Retrieval-Augmented LLMs (RAG)\n",
    "\n",
    "This notebook demonstrates how to compute common retrieval metrics—Precision@k, Recall@k, Hit@k, Mean Reciprocal Rank (MRR), and normalized Discounted Cumulative Gain (nDCG)—on a small simulated dataset. These metrics help assess how well a retrieval system surfaces relevant documents for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b37db",
   "metadata": {},
   "source": [
    "## Simulated Dataset\n",
    "\n",
    "We simulate three queries with sets of relevant documents. Two models (A and B) return ranked lists of candidate documents. We compute metrics for the top **k=3** retrieved items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# Queries and ground-truth relevant documents\n",
    "queries = ['q1', 'q2', 'q3']\n",
    "relevant_docs = {\n",
    "    'q1': {'doc1', 'doc3', 'doc4'},\n",
    "    'q2': {'doc2', 'doc5'},\n",
    "    'q3': {'doc3', 'doc6', 'doc7', 'doc9'}\n",
    "}\n",
    "\n",
    "# Retrieval results for two models\n",
    "predictions_A = {\n",
    "    'q1': ['doc1', 'doc2', 'doc3', 'doc4', 'doc5'],\n",
    "    'q2': ['doc2', 'doc5', 'doc6', 'doc1', 'doc3'],\n",
    "    'q3': ['doc4', 'doc3', 'doc7', 'doc2', 'doc8', 'doc6', 'doc9']\n",
    "}\n",
    "\n",
    "predictions_B = {\n",
    "    'q1': ['doc2', 'doc4', 'doc5', 'doc1', 'doc3'],\n",
    "    'q2': ['doc5', 'doc6', 'doc2', 'doc3', 'doc1'],\n",
    "    'q3': ['doc9', 'doc6', 'doc7', 'doc3', 'doc8', 'doc2']\n",
    "}\n",
    "\n",
    "\n",
    "def precision_at_k(predicted, relevant, k):\n",
    "    return sum(doc in relevant for doc in predicted[:k]) / k\n",
    "\n",
    "def recall_at_k(predicted, relevant, k):\n",
    "    return sum(doc in relevant for doc in predicted[:k]) / len(relevant)\n",
    "\n",
    "def hit_at_k(predicted, relevant, k):\n",
    "    return 1.0 if any(doc in relevant for doc in predicted[:k]) else 0.0\n",
    "\n",
    "def mrr(predicted, relevant):\n",
    "    for idx, doc in enumerate(predicted, start=1):\n",
    "        if doc in relevant:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n",
    "def dcg(predicted, relevant):\n",
    "    return sum(1.0 / math.log2(i + 2) for i, doc in enumerate(predicted) if doc in relevant)\n",
    "\n",
    "def idcg(relevant):\n",
    "    return sum(1.0 / math.log2(i + 2) for i in range(len(relevant)))\n",
    "\n",
    "def ndcg(predicted, relevant):\n",
    "    denom = idcg(relevant)\n",
    "    return dcg(predicted, relevant) / denom if denom > 0 else 0.0\n",
    "\n",
    "k = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d45ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Model  Precision@3  Recall@3  Hit@3   MRR  nDCG\n",
       "Model A        0.667     0.722    1.0 0.833 0.872\n",
       "Model B        0.667     0.694    1.0 0.833 0.866"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compute metrics for each model\n",
    "models = {'Model A': predictions_A, 'Model B': predictions_B}\n",
    "metrics = []\n",
    "for model_name, preds in models.items():\n",
    "    p_sum = r_sum = hit_sum = mrr_sum = ndcg_sum = 0\n",
    "    for q in queries:\n",
    "        rel = relevant_docs[q]\n",
    "        res = preds[q]\n",
    "        p_sum += precision_at_k(res, rel, k)\n",
    "        r_sum += recall_at_k(res, rel, k)\n",
    "        hit_sum += hit_at_k(res, rel, k)\n",
    "        mrr_sum += mrr(res, rel)\n",
    "        ndcg_sum += ndcg(res, rel)\n",
    "    n = len(queries)\n",
    "    metrics.append({\n",
    "        'Model': model_name,\n",
    "        f'Precision@{k}': round(p_sum / n, 3),\n",
    "        f'Recall@{k}': round(r_sum / n, 3),\n",
    "        f'Hit@{k}': round(hit_sum / n, 3),\n",
    "        'MRR': round(mrr_sum / n, 3),\n",
    "        'nDCG': round(ndcg_sum / n, 3)\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b37da4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "* **Precision@k** measures the fraction of the top-k retrieved documents that are relevant.\n",
    "* **Recall@k** measures the fraction of relevant documents that appear in the top-k.\n",
    "* **Hit@k** is a binary metric indicating whether any relevant documents are retrieved in the top-k.\n",
    "* **MRR** (mean reciprocal rank) rewards retrieving a relevant document early in the ranking.\n",
    "* **nDCG** accounts for the position of all relevant documents, giving higher weight to those at the top.\n",
    "\n",
    "In this simulation, both models retrieve at least one relevant document in the top 3 for every query, but **Model A** has slightly higher recall and nDCG than **Model B**.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is part of a series on evaluating LLMs. You can find the accompanying blog post and other notebooks in the repository."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
