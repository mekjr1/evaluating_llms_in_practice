{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fbb11c",
   "metadata": {},
   "source": [
    "# Part 5: A/B Testing for LLM Evaluation\n",
    "\n",
    "This notebook illustrates how to design and analyse a simple A/B test to compare two versions of a language model. We simulate user interactions and apply a two‑proportion z‑test to determine whether observed differences in satisfaction are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a780c",
   "metadata": {},
   "source": [
    "## Simulating User Feedback\n",
    "\n",
    "We randomly assign 1,000 users to two models (A and B). Each user either rates the model’s response as satisfactory (1) or unsatisfactory (0). In our simulation, Model A has a true satisfaction rate of 60 % and Model B has 55 %. The code below generates the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55aec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 1000\n",
    "assignments = np.random.choice(['A', 'B'], size=N)\n",
    "# True satisfaction rates\n",
    "p_A_true = 0.6\n",
    "p_B_true = 0.55\n",
    "outcomes = np.where(assignments == 'A', np.random.binomial(1, p_A_true, N), np.random.binomial(1, p_B_true, N))\n",
    "\n",
    "# Count per group\n",
    "n_A = np.sum(assignments == 'A')\n",
    "n_B = np.sum(assignments == 'B')\n",
    "success_A = np.sum((assignments == 'A') & (outcomes == 1))\n",
    "success_B = np.sum((assignments == 'B') & (outcomes == 1))\n",
    "\n",
    "# Observed rates\n",
    "p_A_obs = success_A / n_A\n",
    "p_B_obs = success_B / n_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acc50e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Users_A': 490, 'Users_B': 510, 'Success_A': 296, 'Success_B': 282, 'Observed_rate_A': 0.604, 'Observed_rate_B': 0.553, 'Difference': 0.051, 'z_stat': 1.637, 'p_value': 0.1016}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Difference in observed success rates\n",
    "diff = p_A_obs - p_B_obs\n",
    "\n",
    "# Pooled proportion and standard error\n",
    "p_pool = (success_A + success_B) / (n_A + n_B)\n",
    "se = math.sqrt(p_pool * (1 - p_pool) * (1/n_A + 1/n_B))\n",
    "\n",
    "# z statistic for two-sample proportion test\n",
    "z = diff / se\n",
    "\n",
    "# Two-sided p-value (using error function for normal CDF)\n",
    "p_value = 2 * (1 - (0.5 * (1 + math.erf(abs(z) / math.sqrt(2)))))\n",
    "\n",
    "results = {\n",
    "    'Users_A': n_A,\n",
    "    'Users_B': n_B,\n",
    "    'Success_A': success_A,\n",
    "    'Success_B': success_B,\n",
    "    'Observed_rate_A': round(p_A_obs, 3),\n",
    "    'Observed_rate_B': round(p_B_obs, 3),\n",
    "    'Difference': round(diff, 3),\n",
    "    'z_stat': round(z, 3),\n",
    "    'p_value': round(p_value, 4)\n",
    "}\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1791a324",
   "metadata": {},
   "source": [
    "### Interpreting the A/B Test\n",
    "\n",
    "* **Observed satisfaction rates:** Model A users were satisfied `60.4 %` of the time, while Model B users were satisfied `55.3 %` of the time.\n",
    "* **Difference:** The observed gap in satisfaction is `5.1 percentage points`.\n",
    "* **z‑statistic:** `z = 1.64`.  This measures how many standard errors the observed difference is from zero.\n",
    "* **p-value:** `p = 0.1016`.  A p‑value below 0.05 would typically suggest the difference is statistically significant.\n",
    "\n",
    "In this simulation, the p-value is above, indicating that the higher satisfaction rate for Model A is not statistically significant at the 5 % level.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook accompanies an article on A/B testing for language models. It provides a simple template for designing experiments and computing significance. Real evaluations should consider user segmentation, effect sizes, and ethical constraints."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
