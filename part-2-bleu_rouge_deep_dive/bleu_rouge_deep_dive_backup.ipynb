{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab8d50b",
   "metadata": {},
   "source": [
    "# Scoring Summaries with LLMs: BLEU and ROUGE in Action\n",
    "\n",
    "## Evaluating LLMs in Practice ‚Äî Part 2 of 7\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mekjr1/evaluating_llms_in_practice/blob/master/part-2-bleu_rouge_deep_dive/bleu_rouge_deep_dive.ipynb?hl=en#runtime_type=gpu)\n",
    "\n",
    "üìå **Recap from Part 1**\n",
    "\n",
    "In Part 1, we saw why accuracy and F1 don't work for LLMs and introduced a range of metrics. This time, we're going deeper into two of the most widely used ones: **BLEU** and **ROUGE**.\n",
    "\n",
    "We'll look at how they work, the math behind them, and run experiments to see where they succeed and fail.\n",
    "\n",
    "üß† **Why BLEU and ROUGE?**\n",
    "\n",
    "Both metrics were created in the early 2000s to address the problem of evaluating machine-generated text without relying only on human judgments.\n",
    "\n",
    "- **BLEU (2002)** ‚Üí Machine translation. Measures **precision**: how much of the candidate output matches the reference text.\n",
    "- **ROUGE (2004)** ‚Üí Summarization. Measures **recall**: how much of the reference text is captured in the candidate output.\n",
    "\n",
    "Both rely on **n-gram overlap** (matching sequences of 1, 2, 3, or 4 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c366d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate rouge-score nltk matplotlib pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b44fe5a",
   "metadata": {},
   "source": [
    "## üìê How BLEU Works\n",
    "\n",
    "The BLEU score is based on two parts:\n",
    "\n",
    "**1. Modified n-gram precision:**\n",
    "\n",
    "$$P_n = \\frac{\\sum_{\\text{ngram} \\in \\text{candidate}} \\min(\\text{count}_{\\text{cand}}, \\text{count}_{\\text{ref}})}{\\sum_{\\text{ngram} \\in \\text{candidate}} \\text{count}_{\\text{cand}}}$$\n",
    "\n",
    "**2. Brevity Penalty (BP):**\n",
    "\n",
    "$$BP = \\begin{cases} \n",
    "1 & \\text{if } \\text{length}_{\\text{cand}} > \\text{length}_{\\text{ref}} \\\\\n",
    "e^{(1-\\frac{\\text{length}_{\\text{ref}}}{\\text{length}_{\\text{cand}}})} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Final BLEU:**\n",
    "\n",
    "$$BLEU = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\cdot \\log P_n\\right)$$\n",
    "\n",
    "Usually $N=4$ and weights $w_n = \\frac{1}{4}$.\n",
    "\n",
    "## üìê How ROUGE Works\n",
    "\n",
    "ROUGE is simpler and comes in several flavors:\n",
    "\n",
    "**ROUGE-N (recall):**\n",
    "\n",
    "$$ROUGE\\_N = \\frac{\\sum_{\\text{ngram} \\in \\text{reference}} \\min(\\text{count}_{\\text{cand}}, \\text{count}_{\\text{ref}})}{\\sum_{\\text{ngram} \\in \\text{reference}} \\text{count}_{\\text{ref}}}$$\n",
    "\n",
    "**ROUGE-L (Longest Common Subsequence):** Measures the longest sequence of words appearing in both candidate and reference.\n",
    "\n",
    "**ROUGE-S (Skip-bigrams):** Measures overlapping pairs of words, allowing gaps.\n",
    "\n",
    "üëâ In practice, **ROUGE-1**, **ROUGE-2**, and **ROUGE-L** are the most common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a0d02",
   "metadata": {},
   "source": [
    "## üìù Toy Example\n",
    "\n",
    "Let's start with a simple example to understand the concepts:\n",
    "\n",
    "**Reference:** \"The cat sat on the mat.\"\n",
    "\n",
    "**Candidate 1:** \"The cat sat on the rug.\"\n",
    "\n",
    "**Candidate 2:** \"A small cat rested quietly.\"\n",
    "\n",
    "Let's see how BLEU and ROUGE score these candidates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d153406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Our toy examples\n",
    "ref_text = \"the cat sat on the mat\"\n",
    "cand1_text = \"the cat sat on the rug\"\n",
    "cand2_text = \"a small cat rested quietly\"\n",
    "\n",
    "# Tokenize for BLEU (expects lists of tokens)\n",
    "reference = [ref_text.split()]  # BLEU expects list of reference lists\n",
    "candidate1 = cand1_text.split()\n",
    "candidate2 = cand2_text.split()\n",
    "\n",
    "print(\"üìö Our Examples:\")\n",
    "print(\"Reference:\", reference[0])\n",
    "print(\"Candidate 1:\", candidate1) \n",
    "print(\"Candidate 2:\", candidate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8bea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BLEU scores with smoothing (to handle zero n-grams)\n",
    "smooth = SmoothingFunction().method1\n",
    "\n",
    "bleu1_cand1 = sentence_bleu(reference, candidate1, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "bleu1_cand2 = sentence_bleu(reference, candidate2, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
    "\n",
    "bleu4_cand1 = sentence_bleu(reference, candidate1, smoothing_function=smooth)  # Default: 4-gram\n",
    "bleu4_cand2 = sentence_bleu(reference, candidate2, smoothing_function=smooth)\n",
    "\n",
    "print(\"üîµ BLEU Scores:\")\n",
    "print(f\"Candidate 1 - BLEU-1: {bleu1_cand1:.3f}, BLEU-4: {bleu4_cand1:.3f}\")\n",
    "print(f\"Candidate 2 - BLEU-1: {bleu1_cand2:.3f}, BLEU-4: {bleu4_cand2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9de087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_cand1 = scorer.score(ref_text, cand1_text)\n",
    "rouge_cand2 = scorer.score(ref_text, cand2_text)\n",
    "\n",
    "print(\"üî¥ ROUGE Scores:\")\n",
    "print(f\"Candidate 1:\")\n",
    "print(f\"  ROUGE-1: {rouge_cand1['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"  ROUGE-2: {rouge_cand1['rouge2'].fmeasure:.3f}\")\n",
    "print(f\"  ROUGE-L: {rouge_cand1['rougeL'].fmeasure:.3f}\")\n",
    "\n",
    "print(f\"Candidate 2:\")\n",
    "print(f\"  ROUGE-1: {rouge_cand2['rouge1'].fmeasure:.3f}\")\n",
    "print(f\"  ROUGE-2: {rouge_cand2['rouge2'].fmeasure:.3f}\")\n",
    "print(f\"  ROUGE-L: {rouge_cand2['rougeL'].fmeasure:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8d83ba",
   "metadata": {},
   "source": [
    "### üí° Observation\n",
    "\n",
    "**Candidate 1**: BLEU ‚âà 0.6, ROUGE ‚âà 0.7 ‚Üí looks good (most words match).\n",
    "\n",
    "**Candidate 2**: BLEU ‚âà 0.1, ROUGE ‚âà 0.2 ‚Üí looks bad, even though semantically it's valid.\n",
    "\n",
    "üëâ This shows why BLEU/ROUGE can be misleading - they miss semantic similarity!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c0afae",
   "metadata": {},
   "source": [
    "## üî¨ Step-by-Step Manual Calculation\n",
    "\n",
    "Let's manually calculate BLEU for \"The cat sat on the rug\" vs \"The cat sat on the mat\" to understand the internals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c2013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_bleu_calculation(reference_tokens, candidate_tokens):\n",
    "    \"\"\"Manually calculate BLEU score to show the internals\"\"\"\n",
    "    \n",
    "    print(f\"üìä Manual BLEU Calculation\")\n",
    "    print(f\"Reference: {reference_tokens}\")\n",
    "    print(f\"Candidate: {candidate_tokens}\")\n",
    "    print()\n",
    "    \n",
    "    # 1-gram precision\n",
    "    ref_1grams = reference_tokens\n",
    "    cand_1grams = candidate_tokens\n",
    "    \n",
    "    matches_1 = 0\n",
    "    ref_counts = {}\n",
    "    for token in ref_1grams:\n",
    "        ref_counts[token] = ref_counts.get(token, 0) + 1\n",
    "    \n",
    "    print(\"1-gram analysis:\")\n",
    "    for token in cand_1grams:\n",
    "        if token in ref_counts and ref_counts[token] > 0:\n",
    "            matches_1 += 1\n",
    "            ref_counts[token] -= 1\n",
    "            print(f\"  '{token}' ‚Üí MATCH ‚úì\")\n",
    "        else:\n",
    "            print(f\"  '{token}' ‚Üí NO MATCH ‚úó\")\n",
    "    \n",
    "    p1 = matches_1 / len(cand_1grams)\n",
    "    print(f\"\\n1-gram precision: {matches_1}/{len(cand_1grams)} = {p1:.3f}\")\n",
    "    \n",
    "    # Brevity penalty\n",
    "    ref_len = len(reference_tokens)\n",
    "    cand_len = len(candidate_tokens)\n",
    "    \n",
    "    if cand_len > ref_len:\n",
    "        bp = 1.0\n",
    "    else:\n",
    "        bp = math.exp(1 - ref_len/cand_len)\n",
    "    \n",
    "    print(f\"\\nBrevity Penalty:\")\n",
    "    print(f\"  Reference length: {ref_len}\")\n",
    "    print(f\"  Candidate length: {cand_len}\")\n",
    "    print(f\"  BP = {bp:.3f}\")\n",
    "    \n",
    "    # Simple BLEU-1 score\n",
    "    bleu_1 = bp * p1\n",
    "    print(f\"\\nüéØ BLEU-1 = BP √ó P1 = {bp:.3f} √ó {p1:.3f} = {bleu_1:.3f}\")\n",
    "    \n",
    "    return bleu_1\n",
    "\n",
    "import math\n",
    "\n",
    "# Manual calculation for our example\n",
    "manual_score = manual_bleu_calculation(\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"rug\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f3178",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 1: Comparing Hugging Face Models\n",
    "\n",
    "Let's scale up to real summarization models and see how they perform on actual news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5893fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# Load a small sample from CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:3]\")\n",
    "\n",
    "# The models we'll compare\n",
    "models = {\n",
    "    \"DistilBART\": \"sshleifer/distilbart-cnn-12-6\",\n",
    "    \"BART-Base\": \"facebook/bart-base\",\n",
    "    \"T5-Small\": \"t5-small\"\n",
    "}\n",
    "\n",
    "print(\"üìä Models to evaluate:\", list(models.keys()))\n",
    "print(\"üì∞ Articles to test:\", len(dataset))\n",
    "print(\"\\nüìù Sample article (first 200 chars):\")\n",
    "print(dataset[0][\"article\"][:200] + \"...\")\n",
    "print(\"\\nüéØ Reference summary:\")\n",
    "print(dataset[0][\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6207871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Store results for all models\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_id in models.items():\n",
    "    print(f\"\\nüîÑ Loading {model_name}...\")\n",
    "    summarizer = pipeline(\"summarization\", model=model_id)\n",
    "    \n",
    "    model_results = {\"Model\": model_name}\n",
    "    bleu_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for i, item in enumerate(dataset):\n",
    "        article = item[\"article\"]\n",
    "        reference = item[\"highlights\"]\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = summarizer(article, max_length=60, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        bleu_score = bleu.compute(predictions=[summary], references=[[reference]])[\"bleu\"]\n",
    "        rouge_score = rouge.compute(predictions=[summary], references=[reference])\n",
    "        \n",
    "        bleu_scores.append(bleu_score)\n",
    "        rouge1_scores.append(rouge_score[\"rouge1\"])\n",
    "        rouge2_scores.append(rouge_score[\"rouge2\"])\n",
    "        rougeL_scores.append(rouge_score[\"rougeL\"])\n",
    "        \n",
    "        print(f\"  üìÑ Article {i+1}: BLEU={bleu_score:.3f}, ROUGE-1={rouge_score['rouge1']:.3f}\")\n",
    "        print(f\"     Summary: {summary[:80]}...\")\n",
    "    \n",
    "    # Average scores\n",
    "    model_results.update({\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"ROUGE-1\": sum(rouge1_scores) / len(rouge1_scores),\n",
    "        \"ROUGE-2\": sum(rouge2_scores) / len(rouge2_scores), \n",
    "        \"ROUGE-L\": sum(rougeL_scores) / len(rougeL_scores)\n",
    "    })\n",
    "    \n",
    "    all_results.append(model_results)\n",
    "\n",
    "# Create results table\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nüìä Final Results:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7962d8",
   "metadata": {},
   "source": [
    "### üèÜ Model Comparison Analysis\n",
    "\n",
    "| Model | BLEU | ROUGE-1 | ROUGE-2 | ROUGE-L | Notes |\n",
    "|-------|------|--------|--------|---------|-------|\n",
    "| DistilBART | ~0.28 | ~0.53 | ~0.38 | ~0.47 | Summarization tuned ‚Üí best |\n",
    "| BART-Base | ~0.06 | ~0.23 | ~0.15 | ~0.18 | Output too long |\n",
    "| T5-Small | ~0.19 | ~0.42 | ~0.29 | ~0.36 | Balanced |\n",
    "\n",
    "*Note: Actual scores may vary based on the specific articles tested.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddecd1",
   "metadata": {},
   "source": [
    "## üî¨ Experiment 2: Short vs Long Summaries\n",
    "\n",
    "BLEU and ROUGE behave differently if one model outputs short concise text while another outputs long verbose summaries. Let's test this hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febfeef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic examples with different lengths\n",
    "reference_summary = \"The president met European leaders in Paris to discuss climate policies.\"\n",
    "\n",
    "# Candidate summaries of different lengths\n",
    "short_summary = \"The president met leaders in Paris.\"\n",
    "medium_summary = \"The president met with European leaders in Paris to discuss policies.\"  \n",
    "long_summary = \"The president of the country met with European leaders in Paris on Tuesday to discuss ongoing climate and economic policies and future cooperation.\"\n",
    "\n",
    "candidates = {\n",
    "    \"Short (7 words)\": short_summary,\n",
    "    \"Medium (11 words)\": medium_summary, \n",
    "    \"Long (21 words)\": long_summary\n",
    "}\n",
    "\n",
    "print(\"üéØ Reference:\", reference_summary)\n",
    "print(f\"   Length: {len(reference_summary.split())} words\\n\")\n",
    "\n",
    "length_results = []\n",
    "\n",
    "for name, candidate in candidates.items():\n",
    "    # BLEU score\n",
    "    ref_tokens = [reference_summary.split()]\n",
    "    cand_tokens = candidate.split()\n",
    "    bleu_score = sentence_bleu(ref_tokens, cand_tokens, smoothing_function=smooth)\n",
    "    \n",
    "    # ROUGE scores\n",
    "    rouge_scores = scorer.score(reference_summary, candidate)\n",
    "    \n",
    "    result = {\n",
    "        \"Summary Type\": name,\n",
    "        \"Length\": len(cand_tokens),\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"ROUGE-2\": rouge_scores['rouge2'].fmeasure,\n",
    "        \"ROUGE-L\": rouge_scores['rougeL'].fmeasure\n",
    "    }\n",
    "    \n",
    "    length_results.append(result)\n",
    "    print(f\"üìè {name}: '{candidate[:50]}...'\")\n",
    "    print(f\"   BLEU: {bleu_score:.3f} | ROUGE-1: {rouge_scores['rouge1'].fmeasure:.3f}\")\n",
    "\n",
    "# Create comparison table\n",
    "length_df = pd.DataFrame(length_results)\n",
    "print(\"\\nüìä Length Impact Results:\")\n",
    "print(length_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b1d0e",
   "metadata": {},
   "source": [
    "### üìè Key Insight\n",
    "\n",
    "- **Short summaries** often get higher BLEU (precision-focused)\n",
    "- **Long summaries** often get higher ROUGE (recall-focused) \n",
    "- But which one is more useful depends on the task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Model comparison chart\n",
    "models_list = results_df['Model'].tolist()\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.2\n",
    "\n",
    "ax1.bar(x - width, results_df['BLEU'], width, label='BLEU', alpha=0.8, color='skyblue')\n",
    "ax1.bar(x, results_df['ROUGE-1'], width, label='ROUGE-1', alpha=0.8, color='lightcoral')\n",
    "ax1.bar(x + width, results_df['ROUGE-L'], width, label='ROUGE-L', alpha=0.8, color='lightgreen')\n",
    "\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('ü§ñ Model Performance: BLEU vs ROUGE')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Length impact chart  \n",
    "lengths = length_df['Length'].tolist()\n",
    "x2 = np.arange(len(lengths))\n",
    "\n",
    "ax2.bar(x2 - width, length_df['BLEU'], width, label='BLEU', alpha=0.8, color='skyblue')\n",
    "ax2.bar(x2, length_df['ROUGE-1'], width, label='ROUGE-1', alpha=0.8, color='lightcoral')\n",
    "ax2.bar(x2 + width, length_df['ROUGE-L'], width, label='ROUGE-L', alpha=0.8, color='lightgreen')\n",
    "\n",
    "ax2.set_xlabel('Summary Length (words)')\n",
    "ax2.set_ylabel('Score')  \n",
    "ax2.set_title('üìè Length Impact: BLEU vs ROUGE')\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(lengths)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. BLEU vs ROUGE correlation\n",
    "all_bleu = list(results_df['BLEU']) + list(length_df['BLEU'])\n",
    "all_rouge1 = list(results_df['ROUGE-1']) + list(length_df['ROUGE-1'])\n",
    "\n",
    "ax3.scatter(all_bleu, all_rouge1, alpha=0.7, s=100, color='purple')\n",
    "ax3.set_xlabel('BLEU Score')\n",
    "ax3.set_ylabel('ROUGE-1 Score')\n",
    "ax3.set_title('üîç BLEU vs ROUGE-1 Correlation')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(all_bleu, all_rouge1, 1)\n",
    "p = np.poly1d(z)\n",
    "ax3.plot(sorted(all_bleu), p(sorted(all_bleu)), \"r--\", alpha=0.8)\n",
    "\n",
    "# 4. Score distribution\n",
    "score_types = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "model_scores = [results_df['BLEU'].mean(), results_df['ROUGE-1'].mean(), \n",
    "                results_df['ROUGE-2'].mean(), results_df['ROUGE-L'].mean()]\n",
    "\n",
    "ax4.pie(model_scores, labels=score_types, autopct='%1.2f', startangle=90, \n",
    "        colors=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "ax4.set_title('üìä Average Score Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insights from the visualizations:\")\n",
    "print(\"‚Ä¢ Different models show varying BLEU vs ROUGE performance\")\n",
    "print(\"‚Ä¢ Summary length significantly affects metric scores\")\n",
    "print(\"‚Ä¢ BLEU and ROUGE-1 often correlate but can diverge\")\n",
    "print(\"‚Ä¢ No single metric tells the complete story!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c3090",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Strengths vs Weaknesses\n",
    "\n",
    "Based on our experiments, let's analyze when BLEU and ROUGE work well and when they fail.\n",
    "\n",
    "### ‚úÖ Strengths\n",
    "\n",
    "- **Fast and reproducible** - Can evaluate thousands of summaries in seconds\n",
    "- **Standardized** - Widely used in research, making comparisons possible  \n",
    "- **No human annotators needed** - Fully automated evaluation\n",
    "- **Good for relative comparisons** - Helps rank different models\n",
    "- **Captures surface-level quality** - Good at detecting completely broken outputs\n",
    "\n",
    "### ‚ùå Weaknesses\n",
    "\n",
    "- **Surface-level only** - Ignores meaning, only matches words\n",
    "- **Sensitive to word choice** - \"car\" vs \"automobile\" scored as completely different\n",
    "- **Length bias** - Short summaries often get higher BLEU; long ones higher ROUGE\n",
    "- **No factual checking** - A summary could score high but contain false information\n",
    "- **Ignores fluency** - Grammatically broken text can still score well if words match\n",
    "- **Single reference limitation** - Real summaries have many valid variations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039de5db",
   "metadata": {},
   "source": [
    "## üß™ Experiment 3: Where BLEU/ROUGE Fail\n",
    "\n",
    "Let's create examples that highlight the limitations of these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e760ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples where BLEU/ROUGE give misleading scores\n",
    "reference = \"The company announced record profits this quarter.\"\n",
    "\n",
    "test_cases = {\n",
    "    \"Perfect Paraphrase\": \"The business reported exceptional earnings this period.\",\n",
    "    \"Factually Wrong\": \"The company announced record losses this quarter.\",\n",
    "    \"Gibberish\": \"Company the announced this record quarter profits.\",\n",
    "    \"Word Salad\": \"Announced profits record company quarter this the.\",\n",
    "    \"Completely Unrelated\": \"Cats love to sleep in sunny windows during afternoon.\"\n",
    "}\n",
    "\n",
    "print(\"üéØ Reference:\", reference)\n",
    "print(\"\\nüß™ Test Cases Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "failure_results = []\n",
    "\n",
    "for case_name, candidate in test_cases.items():\n",
    "    # Calculate scores\n",
    "    ref_tokens = [reference.split()]\n",
    "    cand_tokens = candidate.split()\n",
    "    bleu_score = sentence_bleu(ref_tokens, cand_tokens, smoothing_function=smooth)\n",
    "    rouge_scores = scorer.score(reference, candidate)\n",
    "    \n",
    "    failure_results.append({\n",
    "        \"Case\": case_name,\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-1\": rouge_scores['rouge1'].fmeasure,\n",
    "        \"Human Quality\": \"High\" if case_name == \"Perfect Paraphrase\" else \"Low\"\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìù {case_name}:\")\n",
    "    print(f\"   Text: '{candidate}'\")\n",
    "    print(f\"   BLEU: {bleu_score:.3f} | ROUGE-1: {rouge_scores['rouge1'].fmeasure:.3f}\")\n",
    "    \n",
    "    # Add human judgment\n",
    "    if case_name == \"Perfect Paraphrase\":\n",
    "        print(\"   üë§ Human: Excellent! Perfect meaning, different words.\")\n",
    "    elif case_name == \"Factually Wrong\":\n",
    "        print(\"   üë§ Human: Terrible! Opposite meaning, but high word overlap.\")\n",
    "    elif \"Gibberish\" in case_name or \"Word Salad\" in case_name:\n",
    "        print(\"   üë§ Human: Nonsensical! Same words, wrong order.\")\n",
    "    else:\n",
    "        print(\"   üë§ Human: Completely irrelevant!\")\n",
    "\n",
    "# Summary table\n",
    "failure_df = pd.DataFrame(failure_results)\n",
    "print(\"\\nüìä Failure Analysis Summary:\")\n",
    "print(failure_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932fc0b",
   "metadata": {},
   "source": [
    "### üö® Critical Observations\n",
    "\n",
    "1. **Perfect Paraphrase** gets LOW scores despite being semantically identical\n",
    "2. **Factually Wrong** can get HIGH scores if it uses similar words\n",
    "3. **Gibberish** with same words can score better than good paraphrases\n",
    "4. These metrics are **word-matching**, not **meaning-matching**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1bf211",
   "metadata": {},
   "source": [
    "## üí° Key Takeaways\n",
    "\n",
    "After our deep dive into BLEU and ROUGE, here's what we learned:\n",
    "\n",
    "### üéØ The Core Insight\n",
    "- **BLEU = precision**; **ROUGE = recall**\n",
    "- Both are **n-gram based** and easy to compute\n",
    "- Great for **benchmarking**, but not enough for real-world quality\n",
    "\n",
    "### ‚ö†Ô∏è Major Limitations\n",
    "- They **fail on paraphrases** and don't capture semantics\n",
    "- Can be **gamed** by length manipulation\n",
    "- No understanding of **factual correctness** or **fluency**\n",
    "\n",
    "### üõ†Ô∏è Best Practices\n",
    "- Use them alongside **human evaluation**\n",
    "- Consider **multiple metrics** together\n",
    "- Be aware of **length bias** in your models\n",
    "- Don't rely on them for **final quality judgments**\n",
    "\n",
    "### üîÆ Modern Alternatives\n",
    "For semantic understanding, consider:\n",
    "- **BERTScore** (contextual embeddings)\n",
    "- **BLEURT** (learned evaluation)\n",
    "- **Human evaluation** (still the gold standard)\n",
    "\n",
    "---\n",
    "\n",
    "## üìå What's Next\n",
    "\n",
    "This is **Part 2** of \"Evaluating LLMs in Practice: Metrics, Experiments, and A/B Testing.\"\n",
    "\n",
    "In **Part 3**, we'll dive into **Perplexity**:\n",
    "- What it really measures\n",
    "- Why lower perplexity isn't always better\n",
    "- How to compute it with GPT-2 and GPT-Neo\n",
    "- When perplexity correlates with human quality (and when it doesn't)\n",
    "\n",
    "**Coming up in the series:**\n",
    "- Part 4: Semantic Similarity Metrics (BERTScore, METEOR)\n",
    "- Part 5: Human Evaluation & Inter-Annotator Agreement  \n",
    "- Part 6: A/B Testing for LLM Applications\n",
    "- Part 7: Building Your Own Custom Evaluation Pipeline\n",
    "\n",
    "Stay tuned! ‚≠ê"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
