{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28625610",
   "metadata": {},
   "source": [
    "# Perplexity: How Surprised Is Your Model?\n",
    "\n",
    "This notebook demonstrates how to compute perplexity, a metric that measures how well a language model predicts a sample of text. Lower perplexity indicates the model is less surprised by the text, meaning it predicts the words more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75552f",
   "metadata": {},
   "source": [
    "\n",
    "Perplexity is defined as the exponential of the average negative log-likelihood of a sequence.\n",
    "For a sequence of words \\(w_1, w_2, \\dots, w_N\\), the perplexity of a model with probability distribution \\(P\\) is:\n",
    "\n",
    "\\[ \text{Perplexity} = 2^{ -\f",
    "rac{1}{N} \\sum_{i=1}^N \\log_2 P(w_i | w_{i-1}, \\dots, w_{i-n+1}) } \\]\n",
    "\n",
    "In other words, the lower the perplexity, the better the model predicts the sequence. In the next cell, we build a simple n-gram model to estimate the perplexity on a small corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f537bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:17:53.399203Z",
     "iopub.status.busy": "2025-10-06T14:17:53.398631Z",
     "iopub.status.idle": "2025-10-06T14:17:53.415800Z",
     "shell.execute_reply": "2025-10-06T14:17:53.415141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model Perplexity:\n",
      "Train: 1.122462048309373  Test: 35.49536659755571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_ngram_model(text, n=2):\n",
    "    # Build an n-gram model from the provided text.\n",
    "    model = defaultdict(lambda: defaultdict(int))\n",
    "    words = text.split()\n",
    "    for i in range(n, len(words)):\n",
    "        context = tuple(words[i-n:i])\n",
    "        word = words[i]\n",
    "        model[context][word] += 1\n",
    "    # convert counts to probabilities\n",
    "    for context, counts in model.items():\n",
    "        total = float(sum(counts.values()))\n",
    "        for word in list(counts.keys()):\n",
    "            counts[word] /= total\n",
    "    return model\n",
    "\n",
    "\n",
    "def perplexity(model, text, n=2):\n",
    "    # Compute perplexity of the model on the provided text.\n",
    "    words = text.split()\n",
    "    log_prob = 0.0\n",
    "    count = 0\n",
    "    for i in range(n, len(words)):\n",
    "        context = tuple(words[i-n:i])\n",
    "        word = words[i]\n",
    "        prob = model.get(context, {}).get(word, 1e-6)  # smoothing for unseen words\n",
    "        log_prob += -math.log(prob, 2)\n",
    "        count += 1\n",
    "    return math.pow(2, log_prob / max(count, 1))\n",
    "\n",
    "# Example corpus for training and evaluation\n",
    "train_text = \"hello world it is a nice day hello world it is another beautiful day\"\n",
    "test_text = \"hello world it is a sunny day hello world it is a nice day\"\n",
    "\n",
    "# Build bigram model (n=2)\n",
    "model = build_ngram_model(train_text, n=2)\n",
    "\n",
    "# Compute perplexity on train and test\n",
    "train_perplexity = perplexity(model, train_text, n=2)\n",
    "test_perplexity = perplexity(model, test_text, n=2)\n",
    "\n",
    "print(\"Bigram Model Perplexity:\\nTrain:\", train_perplexity, \" Test:\", test_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "826e991c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-06T14:17:53.419387Z",
     "iopub.status.busy": "2025-10-06T14:17:53.418799Z",
     "iopub.status.idle": "2025-10-06T14:17:53.429973Z",
     "shell.execute_reply": "2025-10-06T14:17:53.428541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load pretrained model or compute perplexity due to: No module named 'transformers'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: Compute perplexity using a pretrained language model from Hugging Face\n",
    "# This cell will attempt to load a small model and compute perplexity on a simple sentence.\n",
    "# If the environment does not have internet or the model cannot be loaded, it will fall back gracefully.\n",
    "try:\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    import torch\n",
    "\n",
    "    model_name = \"distilgpt2\"  # small model for demo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    def calculate_perplexity(text):\n",
    "        encodings = tokenizer(text, return_tensors='pt')\n",
    "        max_length = model.config.n_positions\n",
    "        stride = 512\n",
    "        nlls = []\n",
    "        for i in range(0, encodings.input_ids.size(1), stride):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = i + stride\n",
    "            trg_len = end_loc - i\n",
    "            input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "            target_ids = input_ids.clone()\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "            nlls.append(neg_log_likelihood)\n",
    "        ppl = torch.exp(torch.stack(nlls).sum() / end_loc).item()\n",
    "        return ppl\n",
    "\n",
    "    sample_text = \"Perplexity is a measure of how well a language model predicts text.\"\n",
    "    print(\"Perplexity (distilgpt2):\", calculate_perplexity(sample_text))\n",
    "except Exception as e:\n",
    "    print(\"Could not load pretrained model or compute perplexity due to:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071a627",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion\n",
    "\n",
    "Perplexity gives us a quantitative measure of how well a language model predicts a sequence of text. In this notebook, we implemented a simple n-gram model and calculated perplexity on both training and test texts. We also attempted to use a pretrained Hugging Face model (DistilGPT-2) to compute perplexity on a sample sentence.\n",
    "\n",
    "In practice, lower perplexity values indicate better predictive performance. However, perplexity alone doesn't tell the whole story â€” models can have low perplexity yet produce incoherent or irrelevant outputs. Therefore, perplexity should be used alongside other evaluation metrics and human judgment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
