{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264052c7",
   "metadata": {},
   "source": [
    "# Part 6: Human‑in‑the‑Loop & Preference Models\n",
    "\n",
    "This notebook demonstrates how to simulate human preference data for evaluating language models. Human feedback is often collected in the form of ratings or pairwise comparisons. We simulate pairwise preferences on a set of prompts with multiple raters, aggregate the results, and discuss how to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec34836",
   "metadata": {},
   "source": [
    "## Simulating Pairwise Preferences\n",
    "\n",
    "We simulate 30 prompts, each evaluated by three human raters.  For each prompt there is a true best model (A or B).  If Model A is truly better for a prompt, raters prefer it with probability 0.8; if Model B is truly better, raters prefer A with probability 0.2.  Preferences are encoded as 1 for **A wins** and −1 for **B wins**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a439d50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prompts': 30, 'A_wins': 17, 'B_wins': 13, 'Ties': 0, 'Proportion_A_preferred': 0.567}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "n_prompts = 30\n",
    "n_raters = 3\n",
    "# 60% of prompts favour Model A\n",
    "true_quality = np.random.choice([1, 0], size=n_prompts, p=[0.6, 0.4])\n",
    "\n",
    "# preferences[i,j] = 1 if rater j prefers A on prompt i, -1 if prefers B\n",
    "preferences = np.zeros((n_prompts, n_raters), dtype=int)\n",
    "for i in range(n_prompts):\n",
    "    prob_A = 0.8 if true_quality[i] == 1 else 0.2\n",
    "    for j in range(n_raters):\n",
    "        preferences[i, j] = np.random.choice([1, -1], p=[prob_A, 1 - prob_A])\n",
    "\n",
    "# Aggregate preference per prompt: sign of sum\n",
    "agg_pref = np.sign(preferences.sum(axis=1))\n",
    "A_wins = np.sum(agg_pref == 1)\n",
    "B_wins = np.sum(agg_pref == -1)\n",
    "ties = np.sum(agg_pref == 0)\n",
    "proportion_A = A_wins / (A_wins + B_wins + ties)\n",
    "\n",
    "# Results dictionary\n",
    "results = {\n",
    "    'Prompts': n_prompts,\n",
    "    'A_wins': int(A_wins),\n",
    "    'B_wins': int(B_wins),\n",
    "    'Ties': int(ties),\n",
    "    'Proportion_A_preferred': round(proportion_A, 3)\n",
    "}\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5145d4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "In this simulation, human raters collectively preferred Model A on **17 out of 30 prompts**, with **0** ties.  That corresponds to **56.7 %** of prompts favouring Model A overall.\n",
    "\n",
    "Human‑in‑the‑loop evaluations often use aggregated preferences or ratings to train reward models and to assess whether one model is better than another.  Pairwise comparison data is particularly powerful for **reinforcement learning from human feedback (RLHF)**, where a reward model is trained to predict which of two outputs is preferred.\n",
    "\n",
    "Keep in mind that real human evaluations involve additional complexities: rater biases, annotation guidelines, quality checks, and inter‑rater reliability metrics.  Nevertheless, this toy example shows how to aggregate preferences and quantify which model wins more often.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is part of a series on evaluating LLMs.  See the accompanying article for more context and further discussion."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
