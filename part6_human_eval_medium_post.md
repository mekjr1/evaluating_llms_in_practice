# Humanâ€‘inâ€‘theâ€‘Loop Evaluation & Preference Models  
*Evaluating LLMs in Practice â€” Partâ€¯6 ofÂ 7*

In [Partâ€¯5](./part5_ab_testing_medium_post.md) we compared two language models using an A/B test.  But A/B tests still rely on aggregate metrics like clickâ€‘through or satisfaction.  Sometimes you need **direct human feedback** on the content itself: which answer is better?  How helpful is this summary?  Do users prefer one response over another?  Collecting such feedback and aggregating it into signals for evaluation and training is the subject of this part.

## ğŸ‘©â€âš–ï¸ Why Humanâ€‘inâ€‘theâ€‘Loop?

Automatic metrics (BLEU, ROUGE, perplexity, retrieval metrics) only approximate quality.  Humans, however, can judge **correctness, helpfulness, tone, bias, and context**.  Gathering human judgments has become central to modern languageâ€‘model development:

- **Ratings:** Assign a scalar score (e.g. 1â€“5 stars) to a single output.  Useful for assessing overall quality or fluency.
- **Pairwise comparisons:** Given two outputs for the same prompt, decide which one is better.  This produces relative preferences that are wellâ€‘suited to training **reward models** in *reinforcement learning from human feedback (RLHF)*.

Both approaches require careful annotation guidelines, multiple raters per example, and mechanisms to ensure data quality.

## ğŸ§ª Simulating Pairwise Preferences

To illustrate how pairwise preference data is aggregated, we simulate a small experiment with 30 prompts and three human raters per prompt:

- **True best model:** For 60Â % of the prompts, ModelÂ Aâ€™s response is truly better; for the remaining 40Â %, ModelÂ Bâ€™s is better.
- **Rater behaviour:** If ModelÂ A is truly better, a rater picks A with 0.8 probability; otherwise they pick B with 0.8 probability.

Each rater expresses a preference (1 for A, âˆ’1 for B).  We sum the preferences across raters and take the sign to determine the winner for that prompt (ties are possible if raters disagree evenly).

Running this simulation yields the following aggregate:

| Statistic             | Value |
|----------------------:|------:|
| Prompts               | 30    |
| AÂ wins                | 17    |
| BÂ wins                | 13    |
| Ties                  | 0     |
| AÂ preferred (overall) | 56.7Â %|

In this run, ModelÂ A is favoured on slightly more than half the prompts.  Because we simulated a noisy process, the outcome varies run to run even though A is the true best on 60Â % of the prompts.

## ğŸ’¬ Discussion & Best Practices

- **Multiple raters matter:** Aggregating three or more judgments per prompt reduces noise and allows you to detect when raters disagree.
- **Interâ€‘rater reliability:** Beyond simple majority vote, metrics like Fleissâ€™Â Îº can quantify agreement among annotators.
- **Annotation guidelines:** Clear instructions and examples help raters provide consistent and fair evaluations.
- **Reward models:** In RLHF, a neural network (reward model) is trained on pairwise data to predict human preferences.  This reward model then guides policy optimisation.
- **Bias and fairness:** Human raters may have biases; sampling diverse raters and prompts can mitigate this.  Always monitor for harmful or discriminatory outputs.

â¡ï¸ **Next Part:** Weâ€™ll take a step back and discuss **holistic evaluation** â€” combining multiple metrics, including safety and fairness, to judge language models across tasks.

ğŸ““ *Full notebook and code*: [partâ€‘6â€‘human_eval/human_eval.ipynb](./part-6-human_eval/human_eval.ipynb)
